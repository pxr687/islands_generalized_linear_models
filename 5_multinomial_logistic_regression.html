
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Multinomial Logistic Regression &#8212; Islands - Generalized Linear Models</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="About the author" href="99_about_the_author.html" />
    <link rel="prev" title="Binary Logistic Regression" href="4_binary_logistic_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/islands_main.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Islands - Generalized Linear Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0_main_page.html">
                    Who this book is for
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_generalized_linear_models.html">
   What are Generalized Linear Models?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_poisson_regression.html">
   Poisson Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_binary_logistic_regression.html">
   Binary Logistic Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Multinomial Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_about_the_author.html">
   About the author
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/5_multinomial_logistic_regression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F5_multinomial_logistic_regression.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/5_multinomial_logistic_regression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#context">
   Context
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model">
   Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-estimation">
   Parameter Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation-one-predictor-model">
   Interpretation (one predictor model)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multinomial-logistic-regression-with-multiple-predictors">
   Multinomial Logistic Regression with multiple predictors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation-multiple-predictors">
   Interpretation (multiple predictors)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistical-interaction-in-multinomial-logistic-regression">
   Statistical interaction in Multinomial Logistic Regression
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Multinomial Logistic Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#context">
   Context
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model">
   Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-estimation">
   Parameter Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation-one-predictor-model">
   Interpretation (one predictor model)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multinomial-logistic-regression-with-multiple-predictors">
   Multinomial Logistic Regression with multiple predictors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation-multiple-predictors">
   Interpretation (multiple predictors)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistical-interaction-in-multinomial-logistic-regression">
   Statistical interaction in Multinomial Logistic Regression
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="multinomial-logistic-regression">
<h1>Multinomial Logistic Regression<a class="headerlink" href="#multinomial-logistic-regression" title="Permalink to this headline">#</a></h1>
<br>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/island_mul_log_reg.png"><img alt="_images/island_mul_log_reg.png" class="bg-primary mb-1 align-center" src="_images/island_mul_log_reg.png" style="width: 270px;" /></a>
<br><p>Imagine you visting a new island, where you and a team of social scientists are conducting research about the three different religions that are practiced there. Your team is interested in the relationship between the following variables:</p>
<p><code class="docutils literal notranslate"><span class="pre">income</span></code> - a continuous numerical variable, indicating the annual income, in 1000s, of each participant</p>
<p><code class="docutils literal notranslate"><span class="pre">religion</span></code> - a nominal categorical variable, indicating group membership to one of three religions (<code class="docutils literal notranslate"><span class="pre">Communionism</span></code>, <code class="docutils literal notranslate"><span class="pre">Symmetrianity</span></code> or <code class="docutils literal notranslate"><span class="pre">Lamothianism</span></code>)</p>
<p><code class="docutils literal notranslate"><span class="pre">biological_sex</span></code> - a binary nominal-categorical variable, indicating the the biological sex of each participant</p>
<p>The data from a random sample of islanders is shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">axes3d</span>
<span class="kn">import</span> <span class="nn">islands_GLM</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="c1"># make plots look like R!</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">r_ify</span><span class="p">()</span>

<span class="c1"># generate the data for this page</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">islands_GLM</span><span class="o">.</span><span class="n">data_gen_multinomial</span><span class="p">()</span>

<span class="c1"># show the data</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>income</th>
      <th>religion</th>
      <th>sex</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>78</td>
      <td>Lamothianism</td>
      <td>female</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>Communionism</td>
      <td>male</td>
    </tr>
    <tr>
      <th>2</th>
      <td>55</td>
      <td>Communionism</td>
      <td>female</td>
    </tr>
    <tr>
      <th>3</th>
      <td>186</td>
      <td>Lamothianism</td>
      <td>female</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>Communionism</td>
      <td>male</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>95</th>
      <td>107</td>
      <td>Symmetrianity</td>
      <td>male</td>
    </tr>
    <tr>
      <th>96</th>
      <td>29</td>
      <td>Symmetrianity</td>
      <td>male</td>
    </tr>
    <tr>
      <th>97</th>
      <td>102</td>
      <td>Lamothianism</td>
      <td>female</td>
    </tr>
    <tr>
      <th>98</th>
      <td>22</td>
      <td>Symmetrianity</td>
      <td>male</td>
    </tr>
    <tr>
      <th>99</th>
      <td>107</td>
      <td>Lamothianism</td>
      <td>male</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 3 columns</p>
</div></div></div>
</div>
<p>As with we’ve seen with the categorical variables we’ve dealt with previously, in order to include them in a regression model we need to dummy code them. For the binary categorical variable <code class="docutils literal notranslate"><span class="pre">biological_sex</span></code> we can use 0 and 1 as the dummy codes as previously. However, the variable <code class="docutils literal notranslate"><span class="pre">religion</span></code> has three (rather than two) categories into which observations can fall. It is a <em>multinomial</em> rather than binary variable, as it has more than two categories. Resultantly, we will need more than two values in order to dummy code it (we’ll use <code class="docutils literal notranslate"><span class="pre">Communionism</span> <span class="pre">=</span> <span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">Symmetrianity</span> <span class="pre">=</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">Lamothianism</span> <span class="pre">=</span> <span class="pre">2</span></code>). The code cell below adds some columns to the dataframe containing the dummy codes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate dummy variables</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;religion_dummy&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">df</span><span class="p">[</span><span class="s1">&#39;religion&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;Communionism&#39;</span><span class="p">,</span> <span class="s1">&#39;Symmetrianity&#39;</span><span class="p">,</span> <span class="s1">&#39;Lamothianism&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;sex_dummy&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">df</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># show the data</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>income</th>
      <th>religion</th>
      <th>sex</th>
      <th>religion_dummy</th>
      <th>sex_dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>78</td>
      <td>Lamothianism</td>
      <td>female</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>Communionism</td>
      <td>male</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>55</td>
      <td>Communionism</td>
      <td>female</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>186</td>
      <td>Lamothianism</td>
      <td>female</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>Communionism</td>
      <td>male</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>95</th>
      <td>107</td>
      <td>Symmetrianity</td>
      <td>male</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>96</th>
      <td>29</td>
      <td>Symmetrianity</td>
      <td>male</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>97</th>
      <td>102</td>
      <td>Lamothianism</td>
      <td>female</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>98</th>
      <td>22</td>
      <td>Symmetrianity</td>
      <td>male</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>99</th>
      <td>107</td>
      <td>Lamothianism</td>
      <td>male</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 5 columns</p>
</div></div></div>
</div>
<section id="context">
<h2>Context<a class="headerlink" href="#context" title="Permalink to this headline">#</a></h2>
<p>We use multinomial logistic regression when we are predicting a categorical variable with several categories, such as <code class="docutils literal notranslate"><span class="pre">religion</span></code>.  As with all generalized linear models, the predictor variables can be of any type (quantitative-continuous, quantitative-discrete, nominal-categorical, ordinal-categorical).</p>
<p>Multinomial logistic regression fits a logistic curve for each outcome category, where each curve shows the predicted probability of an observation falling into that category, given the a set of predictor scores. The image below shows three logistic curves, hypothetically representing the probability of falling into one of three categories, for a given score on a predictor variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this code generates the plot below</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">multinomial_illustration</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5_multinomial_logistic_regression_6_0.png" src="_images/5_multinomial_logistic_regression_6_0.png" />
</div>
</div>
<p>For any value of the predictor we could create a barplot depicting the predicted proportion/probability of scores in each category, for that value of the predictor. Around the middle of the predictor axis on the graph above, such a barplot might look like:</p>
<p><img alt="" src="_images/multi_log_reg_bar.jpg" /></p>
<p>This barplot shows a trinomial distribution - it shows the predicted proportion of observations falling into each of the three categories, at a given level of the predictor (a trinomial distribution is a type of multinomial distribution).</p>
<p>From the conditional distribution perspective, multinomial logistic regression works from by fitting a multinomial distribution for every value of the predictor, as shown in the image below:</p>
<p><img alt="" src="_images/multi_log_reg_conditional.png" /></p>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">#</a></h2>
<p>As we’ve just seen, when predicting a multinomial outcome variable, we are interested in the probability of observations falling into one of three or more categories, based on the value of the predictor variable(s). We can denote the probability of each observation (<span class="math notranslate nohighlight">\(y_i\)</span>) falling into category <span class="math notranslate nohighlight">\(j\)</span> as:</p>
<p><span class="math notranslate nohighlight">\( \large \hat{P}_{(y_{i} = j)} = \hat\pi_{ji}\)</span></p>
<p>We use one of the categories as a reference category (we do this because probabilities sum to 1, so if we have <span class="math notranslate nohighlight">\(c\)</span> categories, we only need to predict probabilities for <span class="math notranslate nohighlight">\(c - 1\)</span> categores. We can get the predcited probability for the remaining category by subtracting the other predicted probabilities from 1). The probability of each observation (<span class="math notranslate nohighlight">\(y_i\)</span>) falling into the reference category can be written as:</p>
<p><span class="math notranslate nohighlight">\( \large \hat{P}_{(y_{i} = ref)} = \hat\pi_{\text{(r)}i} \)</span></p>
<p>For every category (<span class="math notranslate nohighlight">\(j\)</span>) apart from the reference category, a separate linear prediction equation predicts the log of the ratio of the probability of falling into that category vs the reference category, for each observation. This is called the log relative risk:</p>
<p><span class="math notranslate nohighlight">\( \Large \hat{y_{ji}} = ln\left(\frac{Pr(Y=j)}{Pr(Y=\text{Reference})}\right)_{i} = b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki}  \)</span></p>
<p>So, each category apart from the reference category gets its own parameter estimates (intercept and slope(s)). Using the probability symbols from above, we can also write this as:</p>
<p><span class="math notranslate nohighlight">\( \large \hat{y_{ji}}  = ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_i = b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki}  \)</span></p>
<p>The individual predictions of an observation being in category <span class="math notranslate nohighlight">\(j\)</span> (<span class="math notranslate nohighlight">\(\hat{y_{j1}}\)</span>), for all <span class="math notranslate nohighlight">\(n\)</span> observations, using <span class="math notranslate nohighlight">\(k\)</span> variables as predictors are:</p>
<div class="math notranslate nohighlight">
\[ \large \hat{y_{j1}} = ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_1 = b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki}  \]</div>
<div class="math notranslate nohighlight">
\[ \large \hat{y_{j2}}  = ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_2 = b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki}  \]</div>
<div class="math notranslate nohighlight">
\[ \large \hat{y_{j3}}  = ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_3 =  b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki} \]</div>
<div class="math notranslate nohighlight">
\[ \dots \]</div>
<div class="math notranslate nohighlight">
\[ \large \hat{y_{jn}}  = ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_n =  b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki}  \]</div>
<p>…where there are <span class="math notranslate nohighlight">\(k\)</span> predictor variables, and <span class="math notranslate nohighlight">\(n\)</span> observations, and where:</p>
<p><span class="math notranslate nohighlight">\(\hat{y_{ji}} \)</span> : is the predicted log relative risk of falling into category <span class="math notranslate nohighlight">\(j\)</span> for a given set of predictor scores, for the <span class="math notranslate nohighlight">\(i\)</span>th observation</p>
<p><span class="math notranslate nohighlight">\(b_0\)</span> : is the intercept term, the predicted value of the outcome variable when all predictors equal 0</p>
<p><span class="math notranslate nohighlight">\(b_1\)</span> : is the slope of the 1st predictor variable</p>
<p><span class="math notranslate nohighlight">\(x_{1i}\)</span> : is the score on the the first predictor variable, for the <span class="math notranslate nohighlight">\(i\)</span>th observation</p>
<p><span class="math notranslate nohighlight">\(b_k\)</span> : is the slope of the <span class="math notranslate nohighlight">\(k\)</span>th predictor variable</p>
<p><span class="math notranslate nohighlight">\(x_{ki}\)</span> : is the score on the <span class="math notranslate nohighlight">\(k\)</span>th predictor variable, for the <span class="math notranslate nohighlight">\(i\)</span>th observation</p>
<p>In matrix form, the model for each of the <span class="math notranslate nohighlight">\(j\)</span> non-reference categories is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large \begin{bmatrix}
           {\hat{y_{j1}} } \\
           {\hat{y_{j2}} } \\
           {\hat{y_{j3}} } \\
           \vdots \\
           {\hat{y_{jn}} }
         \end{bmatrix} = \begin{bmatrix}
           {ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_1 } \\
           {ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_2 } \\
           {ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_3 }  \\
           \vdots \\
           {\ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_n}
         \end{bmatrix} = \begin{bmatrix}
           {1} &amp; {x_{11}} &amp; \dots &amp; {x_{k1}}\\ 
           {1} &amp; {x_{12}} &amp; \dots &amp; {x_{k2}}\\ 
           {1} &amp; {x_{13}} &amp; \dots &amp; {x_{k3}}\\ 
           \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
           {1} &amp; {x_{1n}} &amp; \dots &amp; {x_{kn}} \\ 
         \end{bmatrix} \begin{bmatrix}
           {b^{j}_{0}} \\ 
           \vdots \\
           {b^{j}_{k}} \\ 
         \end{bmatrix} 
\end{split}\]</div>
</section>
<section id="parameter-estimation">
<h2>Parameter Estimation<a class="headerlink" href="#parameter-estimation" title="Permalink to this headline">#</a></h2>
<p>The predicted probability of falling into class <span class="math notranslate nohighlight">\(j\)</span> for observation <span class="math notranslate nohighlight">\(i\)</span> is given by:</p>
<p><span class="math notranslate nohighlight">\( \large \hat\pi_{ji} = \frac{e^{\hat{y_{ji}}}}{1 + e^{\hat{y_{1}}} + ... e^{\hat{y_{ji}}}}  = \frac{e^{\hat{y_{ji}}}}{1 + \sum{e^{\hat{y_{ji}}}}} \)</span></p>
<p>And predicted probability of falling into reference category is given by:</p>
<p><span class="math notranslate nohighlight">\( \large \hat\pi_{\text{(r)}i} = \frac{1}{1 + \sum{e^{\hat{y_{ji}}}}} = 1 - \sum \hat\pi_{ji} \)</span></p>
<p>As with binary logisitc regression, we us a set of indicator variables for each of the <span class="math notranslate nohighlight">\(j\)</span> categories:</p>
<p><span class="math notranslate nohighlight">\(\text{indicator}_{0} = \begin{cases}
  \text{0 if observation is NOT in outcome category 0}\\ 
  \text{1 if observation is in outcome category 0}   
\end{cases}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{indicator}_{1} = \begin{cases}
  \text{0 if observation is NOT in outcome category 1}\\ 
  \text{1 if observation is in outcome category 1}   
\end{cases}\)</span></p>
<div class="math notranslate nohighlight">
\[ \dots \]</div>
<p><span class="math notranslate nohighlight">\(\text{indicator}_{j} = \begin{cases}
  \text{0 if observation is NOT in outcome category j}\\ 
  \text{1 if observation is in outcome category j}   
\end{cases}\)</span></p>
<p>If an observation has a 0 for all the <span class="math notranslate nohighlight">\(j\)</span> indicator variables, it is in the reference category.</p>
<p>The model gets its parameters, by maximizing the likelilhood function:</p>
<p><span class="math notranslate nohighlight">\( \large \prod\limits_{i = 1}^{n} \hat\pi_{0i}^{\text{indicator}_{0i}} \hat\pi_{1i}^{\text{indicator}_{1i}} ...\hat\pi_{ji}^{\text{indicator}_{ji}} \)</span></p>
<p>In practice, the parameter estimates are obtained by minimizing the negative log-likelihood function (much easier for a computer to work with!):</p>
<p><span class="math notranslate nohighlight">\( \large - \sum\limits_{i = 1}^{n} \text{indicator}_{ji} * \large[ \hat{y_{ji}}  - \ln(1 + \sum{e^{\hat{y_{ji}}}} )\large]\)</span></p>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h2>
<p>Returning to the data from the island, your team want to fit a multinomial logistic regression model, predicting <code class="docutils literal notranslate"><span class="pre">religion</span></code> from <code class="docutils literal notranslate"><span class="pre">income</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this code generates the plot below</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">relig_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5_multinomial_logistic_regression_11_0.png" src="_images/5_multinomial_logistic_regression_11_0.png" />
</div>
</div>
<p>The python code below defines a function which implements the negative log-likelihood formula from above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the cost function for multinomial logistic regression</span>
<span class="k">def</span> <span class="nf">mnlogit_cost</span><span class="p">(</span><span class="n">intercept_and_slope</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

    <span class="n">intercept_1</span><span class="p">,</span> <span class="n">slope_1</span><span class="p">,</span> <span class="n">intercept_2</span><span class="p">,</span> <span class="n">slope_2</span><span class="p">,</span> <span class="o">=</span> <span class="n">intercept_and_slope</span>
    
    <span class="n">predicted_log_rel_risk_1</span> <span class="o">=</span> <span class="n">intercept_1</span> <span class="o">+</span> <span class="n">slope_1</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">predicted_log_rel_risk_2</span> <span class="o">=</span> <span class="n">intercept_2</span> <span class="o">+</span> <span class="n">slope_2</span> <span class="o">*</span> <span class="n">x</span>

    <span class="n">y_dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
     
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_dummies</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">predicted_log_rel_risk_1</span> <span class="o">+</span> <span class="n">y_dummies</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">predicted_log_rel_risk_2</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">predicted_log_rel_risk_1</span><span class="p">)</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">predicted_log_rel_risk_2</span><span class="p">)))</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">log_likelihood</span>
</pre></div>
</div>
</div>
</div>
<p>The python cell below tests the the negative log likelihood function with the parameters:</p>
<p><span class="math notranslate nohighlight">\(b^{j}_0\)</span> = 0.1</p>
<p><span class="math notranslate nohighlight">\(b^{j}_1\)</span> = 0.1</p>
<p>for the <code class="docutils literal notranslate"><span class="pre">religion</span> <span class="pre">~</span> <span class="pre">income</span></code> data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># testing the cost function with intercept = .1, slope = .1, for each linear prediction equation</span>
<span class="n">mnlogit_cost</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span><span class="mf">.1</span><span class="p">,</span><span class="mf">.1</span><span class="p">,</span><span class="mf">.1</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;religion_dummy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>383.65330980255266
</pre></div>
</div>
</div>
</div>
<p>The cell below implements code which passes the negative log-likelihood function and some intitial guesses at the parameters, to the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">minimize</a> function, from the <a class="reference external" href="https://scipy.org/">SciPy</a> library.</p>
<p><code class="docutils literal notranslate"><span class="pre">mimimize</span></code> will try various sets of parameters until it finds the parameters which give the lowest value of the function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># finding the paramters which give the minimum value of the cost function</span>
<span class="n">minimize</span><span class="p">(</span><span class="n">mnlogit_cost</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span> <span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;religion_dummy&#39;</span><span class="p">]),</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-16</span><span class="p">)</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 1.3580911 , -0.02791203, -0.78305941,  0.00661693])
</pre></div>
</div>
</div>
</div>
<p>We can see that these are the same parameter estimates obtained by fitting a multinomial logistic regression using the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library (the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> parameter estimates are in the <code class="docutils literal notranslate"><span class="pre">coef</span></code> section of the regression table shown below):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit a multinomial logistic regression using statsmodels</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">mnlogit</span><span class="p">(</span><span class="s1">&#39;religion_dummy ~ income&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># show the regression table</span>
<span class="n">mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.892573
         Iterations 8
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>MNLogit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>religion_dummy</td>  <th>  No. Observations:  </th>  <td>   100</td>  
</tr>
<tr>
  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>  <td>    96</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Thu, 06 Apr 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.1872</td>  
</tr>
<tr>
  <th>Time:</th>                <td>21:32:48</td>     <th>  Log-Likelihood:    </th> <td> -89.257</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -109.82</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.173e-09</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>religion_dummy=1</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>    1.3581</td> <td>    0.439</td> <td>    3.096</td> <td> 0.002</td> <td>    0.498</td> <td>    2.218</td>
</tr>
<tr>
  <th>income</th>           <td>   -0.0279</td> <td>    0.008</td> <td>   -3.417</td> <td> 0.001</td> <td>   -0.044</td> <td>   -0.012</td>
</tr>
<tr>
  <th>religion_dummy=2</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>   -0.7831</td> <td>    0.402</td> <td>   -1.950</td> <td> 0.051</td> <td>   -1.570</td> <td>    0.004</td>
</tr>
<tr>
  <th>income</th>           <td>    0.0066</td> <td>    0.003</td> <td>    2.362</td> <td> 0.018</td> <td>    0.001</td> <td>    0.012</td>
</tr>
</table></div></div>
</div>
<p>The graph below shows the predicted probabilities of falling into each category, as a function of income, using the predicted probabilities from the multinomial logisitc regression model we just fit:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this code generates the plot below</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">scatter_prob_subplots</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5_multinomial_logistic_regression_21_0.png" src="_images/5_multinomial_logistic_regression_21_0.png" />
</div>
</div>
</section>
<section id="interpretation-one-predictor-model">
<h2>Interpretation (one predictor model)<a class="headerlink" href="#interpretation-one-predictor-model" title="Permalink to this headline">#</a></h2>
<p>Note: the intercept and slope are shown under the <code class="docutils literal notranslate"><span class="pre">coef</span></code> heading in the regression table above, the associated <span class="math notranslate nohighlight">\(p\)</span>-values are shown under <code class="docutils literal notranslate"><span class="pre">P&gt;|z|</span></code>.</p>
<p>The coefficients tell us:</p>
<p><span class="math notranslate nohighlight">\(b^{j}_0\)</span> : the intercept, the expected value of the log relative risk of being in category <span class="math notranslate nohighlight">\(j\)</span> for an observation where the predictor equals 0.</p>
<p><span class="math notranslate nohighlight">\(b^{j}_1\)</span>: the predicted change in the  the log relative risk of being in category <span class="math notranslate nohighlight">\(j\)</span>, for a 1-unit increase in the predictor.</p>
<p>Each prediction from the model comes to us on the scale of the log relative risk. So for the <span class="math notranslate nohighlight">\(i\)</span>th observation, the predicted relative risk of falling into category <span class="math notranslate nohighlight">\(j\)</span> is:</p>
<p><span class="math notranslate nohighlight">\( \large \hat{y_{ji}} = ln \left( \frac{\hat\pi_{j}}{\hat\pi_{\text{(r)}}} \right)_i = b_{0}^{j} + b_{1}^{j}x_{1i} + ... + b_{k}^{j}x_{ki}  \)</span></p>
<p>To get the predicted probabilities, for the <span class="math notranslate nohighlight">\(j\)</span> categories, we use the conversion formula:</p>
<p><span class="math notranslate nohighlight">\( \large \hat\pi_{ji} = \frac{e^{\hat{y_{ji}}}}{1 + e^{\hat{y_{1}}} + ... e^{\hat{y_{ji}}}}  = \frac{e^{\hat{y_{ji}}}}{1 + \sum{e^{\hat{y_{ji}}}}} \)</span></p>
<p>And for the predicted probability og the <span class="math notranslate nohighlight">\(i\)</span>th observation falling into the reference category:</p>
<p><span class="math notranslate nohighlight">\( \large \hat\pi_{\text{(r)}i} = \frac{1}{1 + \sum{e^{\hat{y_{ji}}}}} = 1 - \sum \hat\pi_{ji} \)</span></p>
</section>
<section id="multinomial-logistic-regression-with-multiple-predictors">
<h2>Multinomial Logistic Regression with multiple predictors<a class="headerlink" href="#multinomial-logistic-regression-with-multiple-predictors" title="Permalink to this headline">#</a></h2>
<p>In more than two dimensions - e.g. with more than one predictor - the multinomial logistic regression model fits several probability surfaces, one for each outcome cateogory, which show the predicted probability of category membership, given the predictor variables. (In higher dimensions, a manifold is fit).</p>
<p>The graph below shows a multinomial logisitc regression model fit to an outcome with three cateogries, as a function of two continuous predictor variables. Each surface represents the probability of being in the associate category, for any pairing of predictor scores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show how multinomial logistic regression works in multiple dimensions~</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">three_D_model_plot_multinomial</span><span class="p">(</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="s1">&#39;sex_dummy&#39;</span><span class="p">,</span> 
                   <span class="s1">&#39;religion_dummy&#39;</span><span class="p">,</span> <span class="mf">2.5185199239560534</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.030011667415127917</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0302006835581514</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5366928830869722</span><span class="p">,</span> <span class="mf">0.008857262577209785</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7421731046952662</span><span class="p">,</span>
                  <span class="n">df</span><span class="p">,</span> <span class="n">legend_loc</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">wireframe_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5_multinomial_logistic_regression_24_0.png" src="_images/5_multinomial_logistic_regression_24_0.png" />
</div>
</div>
<p>Remember that the data collected on this island contained the following variables:</p>
<p><code class="docutils literal notranslate"><span class="pre">income</span></code>: a continuous numerical variable, indicating the annual income, in 1000s, of each participant</p>
<p><code class="docutils literal notranslate"><span class="pre">religion</span></code>: a nominal categorical variable, indicating group membership to one of three religions: <code class="docutils literal notranslate"><span class="pre">Communionism</span></code>, <code class="docutils literal notranslate"><span class="pre">Symmetrianity</span></code> or <code class="docutils literal notranslate"><span class="pre">Lamothianism</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">biological_sex</span></code>: a binary nominal-categorical variable, indicating the the biological sex of each participant</p>
<p>We’ll now fit a multinomial logisitc regression model predicting <code class="docutils literal notranslate"><span class="pre">religion</span></code> from <code class="docutils literal notranslate"><span class="pre">income</span></code> and <code class="docutils literal notranslate"><span class="pre">biological_sex</span></code>. The model is <code class="docutils literal notranslate"><span class="pre">religion</span> <span class="pre">~</span> <span class="pre">income</span> <span class="pre">+</span> <span class="pre">biological_sex</span></code>.</p>
<p>The dataframe is shown again below, it already contains a dummy variable for <code class="docutils literal notranslate"><span class="pre">biological_sex</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the dataframe</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>income</th>
      <th>religion</th>
      <th>sex</th>
      <th>religion_dummy</th>
      <th>sex_dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>78</td>
      <td>Lamothianism</td>
      <td>female</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>Communionism</td>
      <td>male</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>55</td>
      <td>Communionism</td>
      <td>female</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>186</td>
      <td>Lamothianism</td>
      <td>female</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>Communionism</td>
      <td>male</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>95</th>
      <td>107</td>
      <td>Symmetrianity</td>
      <td>male</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>96</th>
      <td>29</td>
      <td>Symmetrianity</td>
      <td>male</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>97</th>
      <td>102</td>
      <td>Lamothianism</td>
      <td>female</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>98</th>
      <td>22</td>
      <td>Symmetrianity</td>
      <td>male</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>99</th>
      <td>107</td>
      <td>Lamothianism</td>
      <td>male</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 5 columns</p>
</div></div></div>
</div>
<p>We’ll use the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library to fit the new model <code class="docutils literal notranslate"><span class="pre">religion</span> <span class="pre">~</span> <span class="pre">income</span> <span class="pre">+</span> <span class="pre">biological_sex</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit a multinomial logistic regression model using statsmodels</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">mnlogit</span><span class="p">(</span><span class="s1">&#39;religion_dummy ~ income + sex_dummy&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># show the regression table</span>
<span class="n">mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.856044
         Iterations 8
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>MNLogit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>religion_dummy</td>  <th>  No. Observations:  </th>  <td>   100</td>  
</tr>
<tr>
  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>  <td>    94</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     4</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Thu, 06 Apr 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.2205</td>  
</tr>
<tr>
  <th>Time:</th>                <td>21:32:49</td>     <th>  Log-Likelihood:    </th> <td> -85.604</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -109.82</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>7.666e-10</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>religion_dummy=1</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>    0.2239</td> <td>    0.631</td> <td>    0.355</td> <td> 0.723</td> <td>   -1.012</td> <td>    1.460</td>
</tr>
<tr>
  <th>income</th>           <td>   -0.0228</td> <td>    0.008</td> <td>   -2.809</td> <td> 0.005</td> <td>   -0.039</td> <td>   -0.007</td>
</tr>
<tr>
  <th>sex_dummy</th>        <td>    1.3944</td> <td>    0.613</td> <td>    2.275</td> <td> 0.023</td> <td>    0.193</td> <td>    2.596</td>
</tr>
<tr>
  <th>religion_dummy=2</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>   -0.6714</td> <td>    0.457</td> <td>   -1.469</td> <td> 0.142</td> <td>   -1.567</td> <td>    0.224</td>
</tr>
<tr>
  <th>income</th>           <td>    0.0065</td> <td>    0.003</td> <td>    2.317</td> <td> 0.021</td> <td>    0.001</td> <td>    0.012</td>
</tr>
<tr>
  <th>sex_dummy</th>        <td>   -0.2678</td> <td>    0.533</td> <td>   -0.502</td> <td> 0.616</td> <td>   -1.313</td> <td>    0.778</td>
</tr>
</table></div></div>
</div>
<p>As with the other generalized linear models with have seen with two predictors, we can show the model in 3D dataspace. The graph below, on the left hand plot, shows the probability surfaces for each outcome category, derived from the model. The plot on the right shows the raw data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this code generates the plot below</span>
<span class="n">intercept_1</span><span class="p">,</span> <span class="n">income_slope_1</span><span class="p">,</span> <span class="n">biological_sex_dummy_slope_1</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span>
<span class="n">intercept_2</span><span class="p">,</span> <span class="n">income_slope_2</span><span class="p">,</span> <span class="n">biological_sex_dummy_slope_2</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">][:]</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">three_D_model_plot_multinomial</span><span class="p">(</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="s1">&#39;sex_dummy&#39;</span><span class="p">,</span> 
                   <span class="s1">&#39;religion_dummy&#39;</span><span class="p">,</span> <span class="n">intercept_1</span><span class="p">,</span> <span class="n">income_slope_1</span><span class="p">,</span> <span class="n">biological_sex_dummy_slope_1</span><span class="p">,</span> <span class="n">intercept_2</span><span class="p">,</span> <span class="n">income_slope_2</span><span class="p">,</span> <span class="n">biological_sex_dummy_slope_2</span><span class="p">,</span>
                  <span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5_multinomial_logistic_regression_30_0.png" src="_images/5_multinomial_logistic_regression_30_0.png" />
</div>
</div>
</section>
<section id="interpretation-multiple-predictors">
<h2>Interpretation (multiple predictors)<a class="headerlink" href="#interpretation-multiple-predictors" title="Permalink to this headline">#</a></h2>
<p>Note: the intercept and slopes are shown under the <code class="docutils literal notranslate"><span class="pre">coef</span></code> heading in the regression table above, the associated <span class="math notranslate nohighlight">\(p\)</span>-values are shown under <code class="docutils literal notranslate"><span class="pre">P&gt;|z|</span></code>.</p>
<p>The intercept, for a given category, tells us the predicted log relative risk of being in that category, for an observation whose score on all other predictors was 0. The <span class="math notranslate nohighlight">\(p\)</span>-value for the intercept tells us how (un)likely it would be to obtain an intercpet of the value we obtained, if randomly sampling from a population where the true intercept was 0.</p>
<p>The slope, for a given category, of each predictor tells us the predicted difference in log relative risk of being in that category between scores for two hypothetical observations which differed <em>only</em> by a one unit score in that predictor, controlling for the other variables in the model. The <span class="math notranslate nohighlight">\(p\)</span>-values for each predictor tell us how (un)likely it would be, under repeated random sampling, to observe a slope of the size we observed if the population slope was zero.</p>
</section>
<section id="statistical-interaction-in-multinomial-logistic-regression">
<h2>Statistical interaction in Multinomial Logistic Regression<a class="headerlink" href="#statistical-interaction-in-multinomial-logistic-regression" title="Permalink to this headline">#</a></h2>
<p>The definition of statistical interaction is: the influence of a predictor on the outcome variable depends on the value of some other predictor(s).</p>
<p>If we fit an interaction term in a multinomial logistic regression model, then this allows each logistic regression probability surfaces to ‘bend’, so that the effect of one predictor depends on the value of the other predictor. The graph below shows this for one logistic probability surface:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this code generates the plot below</span>
<span class="n">islands_GLM</span><span class="o">.</span><span class="n">three_D_logistic_plot</span><span class="p">(</span><span class="n">interaction</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5_multinomial_logistic_regression_33_0.png" src="_images/5_multinomial_logistic_regression_33_0.png" />
</div>
</div>
<p>That’s it for multinomial logistic regression (for now). You can visit another page/island via the links in the table of contents, and at the bottom of this page…</p>
<hr class="docutils" />
<p>By <a class="reference internal" href="99_about_the_author.html"><span class="doc std std-doc">pxr687</span></a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="4_binary_logistic_regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Binary Logistic Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="99_about_the_author.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">About the author</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>