{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713c99f",
   "metadata": {},
   "source": [
    "###  Islands: Generalized Linear Models - Chapter 1\n",
    "\n",
    "[Back to Main Page](0_main_page.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<h1> <center> What are Generalized Linear Models\n",
    "    ? </center> </h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dfeca",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "Generalized linear models are extensions of the linear regression model that relax some of the assumptions of linear regression. As a result generalized linear models can be applied to a wider variety of datasets. Generalized linear models are best understood by reference to their foundation: linear regression, which we will now recap. \n",
    "\n",
    "Linear regression involves modelling the linear relationship between a set of scores on a quantitative outcome variable (denoted $y$) and scores on a set of predictor variables (denoted with $x_1, x_2 ... x_k$). The predictor variables can be of any type (quantitative-continuous, quantitative-discrete, nominal-categorical, ordinal-categorical). Linear regression uses a linear prediction equation of the form:\n",
    "\n",
    "$\\large \\hat{y}_i = b_0 + b_1x_{1i} ... + b_kx_{ki} $\n",
    "\n",
    "where:\n",
    "\n",
    "$\\hat{y_i} $ : is the predicted value of the outcome variable for a given set of predictor scores, for the $i$th observation\n",
    "\n",
    "$b_0$ : is the intercept term, the predicted value of the outcome variable when all predictors equal 0\n",
    "\n",
    "$b_1$ : is the slope of the 1st predictor variable\n",
    "\n",
    "$x_{1i}$ : is the score on the the first predictor variable, for the $i$th observation\n",
    "\n",
    "$b_k$ : is the slope of the $k$th predictor variable\n",
    "\n",
    "$x_{ki}$ : is the score on the $k$th predictor variable, for the $i$th observation\n",
    "\n",
    "Linear regression assumes a normal distribution of the residuals - that is, that the distribution of the differences between the model predictions and the actual datapoints $(y_i - \\hat{y_i})$ is normal. Generalized linear models relax this (and other) assumptions, and so extend the machinery of linear regression to allow us to use a linear prediction equation to predict an outcome variable with a response distribution that is non-normal.\n",
    "\n",
    "## The 'Conditional Distribution' view of regression models\n",
    "\n",
    "Linear regression is often introduced via the sum of squares perspective, whereby the parameter estimates (the intercept and slopes) are obtained by minimizing:\n",
    "\n",
    "$ \\large \\sum\\limits_{i = 1}^{n} (y_i - \\hat{y}_i)$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$ \\large \\sum\\limits_{i = 1}^{n} (y_i - (b_0 + b_1x_{1i} ... + b_kx_{ki})) $\n",
    "\n",
    "Minimizing the sum of the squared error works for linear regression, but will not provide the best parameter estimates for other generalized linear models. A different perspective therefore is required to understand parameter estimation in generalized linear models. I will refer to this as the 'conditional distribution' perspective. This is easiest to visualize for a linear model containing only one predictor. \n",
    "\n",
    "The conditional distribution perspective is that linear regression fits a normal distribution for each level of the predictor, and that the linear regression line runs through the mean of each normal distribution, as shown in the image below:\n",
    "\n",
    "\n",
    "![](images/GLM_normal_identity.png)\n",
    "(Image from: https://blogs.sas.com/content/iml/2015/09/10/plot-distrib-reg-model.html)\n",
    "\n",
    "If the predictor is *useful* - e.g. it gives a lot of predictive information about the outcome variable - then the conditional means of these normal distributions will change a lot as the value of the predictor changes. The image below illustrates a strong predictor-outcome relationship in a single-predictor linear regression. The conditional mean of the red distribution (high value of the predictor) is much larger than the mean of the orange distribution (low values of the predictor):\n",
    "\n",
    "![](images/linear_regression_ML.png)\n",
    "\n",
    "If the predictor is not very useful, then the conditional means of the normal distributions will not change a lot as the value of the predictor changes (e.g. the predictor gives very little information about the value of the outcome variable). The image below illustrates a weak/null predictor-outcome relationship in a single-predictor linear regression. The conditional mean of the red distribution (high value of the predictor) is the same as the mean of the orange distribution (low values of the predictor):\n",
    "\n",
    "![](images/linear_regression_ML_null.png)\n",
    "\n",
    "For linear regression, the parameters estimates produced via the conditional distribution approach and via the sum of squares approach are equivalent. Generalized linear models, other than linear regression, work by allowing other distributional forms (other than normal distributions) to be fit to the outcome variable. In later pages, we'll go into detail about the how these distributions are fit to the data, from the conditional distribution perspective (technically called the *maximum likelihood* perspective). \n",
    "\n",
    "Some other ways of writing the linear regression are useful here. Because the linear regression model predicts the conditional mean as a function of the predictor variables, we can write the model as:\n",
    "\n",
    "$\\large \\mu_i = b_0 + b_1x_{1i} ... + b_kx_{ki} $\n",
    "\n",
    "Generalized linear models -of which the linear regression model is a special case - fit a linear regression equation to *some function of the conditional mean*. This function is called the *link function* as it links the linear prediction equation with the outcome variable. In the case of linear regression, the link function is just $1 *\\mu_i $, and so is referred to as the *identity link function*. Thus, the general form of generalized linear models is:\n",
    "\n",
    "$\\large f(\\mu_i) = b_0 + b_1x_{1i} ... + b_kx_{ki} $\n",
    "\n",
    "For linear regression, the predictions the model can produce range across the entire real line $(-∞, ∞)$. This would produce nonsensical predictions for certain types of outcome variable. For instance, negative valued predictions do not make sense for binary outcomes that fall into one of two categories, and are dummy-coded as 0 or 1. Nor do negative predictions make sense for count outcomes, which take only whole-number positive values.\n",
    "\n",
    "The link function in generalized linear models maps the linear prediction equation to a function of the conditional mean which ranges from $(-∞, ∞)$. For instance, when dealing with binary outcome variables, we can use the logit link function $ln(\\frac{μ} {1 – μ})$. This maps the linear prediction equation to the log of the odds ratio, the scale of which ranges from $(-∞, ∞)$. The predictions of the logistic regression model form a straight line on the scale of the log odds ratio, but form a different curve on the scale of the original data.\n",
    "\n",
    "The table below shows some of the generalized linear models that are covered by this textbook:\n",
    "\n",
    "| Model type       | Response distribution             | Range of predicted values               | Link name        | Link function         |\n",
    "|--------------------------|--------------------------|------------------------|------------------|-----------------------|\n",
    "| Linear Regression        | Normal Distribution      | real: (-∞, ∞)          | Identity         | 1 * $μ$                   |\n",
    "| Logistic Regression      | Bernoulli                | Integers: {0, 1}       | Logit          | $ln(\\frac{μ} {1 – μ})$ |\n",
    "| Poisson Regression       | Poisson Distribution     | integers: 0, 1, 2, …   | Log              | $ln(μ) $              |\n",
    "\n",
    "## The structure of this book\n",
    "\n",
    "This book will use simulated data, which cleanly illustrates the principles of fitting generalized linear models. Each page will focus on a different type of generalized linear model, and will involve data from a different hypothetical island. Each model will be explained first in the one-predictor case, using a continuous predictor, then in using several predictors, one of which will be categorical. \n",
    "\n",
    "Let's visit the islands, the links are below..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a676e5",
   "metadata": {},
   "source": [
    "## Other Chapters\n",
    "\n",
    "1. [What are Generalized Linear Models?](1_generalized_linear_models.ipynb)\n",
    "2. [Linear Regression](2_linear_regression.ipynb)\n",
    "3. [Poisson Regression](3_poisson_regression.ipynb)\n",
    "4. [Binary Logistic Regression](4_binary_logistic_regression.ipynb)\n",
    "5. [Multinomial Logistic Regression](5_multinomial_logistic_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342767c",
   "metadata": {},
   "source": [
    "***\n",
    "By [pxr687](99_about_the_author.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
