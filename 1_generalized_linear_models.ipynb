{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Islands: Generalized Linear Models - Chapter 1\n",
    "\n",
    "[Back to Main Page](0_main_page.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<h1> <center> What are Generalized Linear Models\n",
    "    ? </center> </h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "Generalized linear models are extensions of the linear regression model that relax some of the assumptions of linear regression, and as a result can be applied to a wider variety of datasets. Linear regression involves modelling the linear relation between a scores on a quantitative outcome variable ($y$) and scores on a set of predictor variables ($x_1, x_2 ... x_k$). The predictor variables can be of any type (quantitative-continuous, quantitative-discrete, nominal-categorical, ordinal-categorical). Linear regression uses a linear prediction equation of the form:\n",
    "\n",
    "$\\large \\hat{y}_i = b_0 + b_1x_{1i} ... + b_kx_{ki} $\n",
    "\n",
    "where:\n",
    "\n",
    "$\\hat{y_i} $ : is the predicted value of the outcome variable for a given set of predictor scores, for the $i$th observation\n",
    "\n",
    "$b_0$ : is the intercept term, the predicted value of the outcome variable when all predictors equal 0\n",
    "\n",
    "$b_1$ : is the slope of the 1st predictor variable\n",
    "\n",
    "$x_{1i}$ : is the score on the the first predictor variable, for the $i$th observation\n",
    "\n",
    "$b_k$ : is the slope of the $k$th predictor variable\n",
    "\n",
    "$x_{ki}$ : is the score on the $k$th predictor variable, for the $i$th observation\n",
    "\n",
    "Linear regression assumes a normal distribution of the residuals - the differences between the model predictions and the actual datapoints $(y_i - \\hat{y_i})$. Generalized linear models relax this (and other) assumptions, and so extend the machinery of linear regression to allow us to use a linear prediction equation to predict a response distribution that is non-normal.\n",
    "\n",
    "## The 'Conditional Distribution' view of regression models\n",
    "\n",
    "Linear regression is often introduced via the sum of squares perspective, whereby the parameter estimates (the intercept and slopes) are obtained by minimizing:\n",
    "\n",
    "$ \\large \\sum\\limits_{i = 1}^{n} (y_i - \\hat{y}_i)$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$ \\large \\sum\\limits_{i = 1}^{n} (y_i - (b_0 + b_1x_{1i} ... + b_kx_{ki})) $\n",
    "\n",
    "This works for linear regression, but a different perspective is required to understand parameter estimation in generalized linear models. I will refer to this as the 'conditional distribution' perspective. This is easiest to visualize for a linear model containing only one predictor. \n",
    "\n",
    "The conditional distribution perspective is that linear regression fits a normal distribution for each level of the predictor, and that the linear regression line runs through the mean of each normal distribution:\n",
    "\n",
    "\n",
    "![](images/GLM_normal_identity.png)\n",
    "(Image from: https://blogs.sas.com/content/iml/2015/09/10/plot-distrib-reg-model.html)\n",
    "\n",
    "If the predictor is useful, then the conditional means will change a lot as the value of the predictor changes (e.g. the predictor gives a lot of information about the value of the outcome variable):\n",
    "\n",
    "![](images/linear_regression_ML.png)\n",
    "\n",
    "If the predictor is not very useful, then the conditional means will not change a lot as the value of the predictor changes (e.g. the predictor gives very little information about the value of the outcome variable):\n",
    "\n",
    "![](images/linear_regression_ML_null.png)\n",
    "\n",
    "We'll come onto the how these distributions are fit in later pages. For linear regression, the parameters estimates produced via the conditional distribution approach and via the sum of squares approach are equivalent. \n",
    "\n",
    "Generalized linear models work by allowing other distributional forms (other than normal distributions) to be fit to the outcome variable.\n",
    "\n",
    "Some other ways of writing the linear regression are useful here. Because the linear regression model predicts the conditional mean as a function of the predictor variables, we can write the model as:\n",
    "\n",
    "$\\large \\mu_i = b_0 + b_1x_{1i} ... + b_kx_{ki} $\n",
    "\n",
    "Generalized linear models -of which the linear regression model is a special case - predict *some function of the conditional mean*. In the case of linear regression, this is function is just $1 *\\mu $, and so is referred to as the *identity link function*. Thus, the general form of generalized linear models is:\n",
    "\n",
    "$\\large f(\\mu_i) = b_0 + b_1x_{1i} ... + b_kx_{ki} $\n",
    "\n",
    "For linear regression, the predictions the model can produce range across the entire real line $(-∞, ∞)$. This would produce nonsensical predictions for certain types of outcome variable. For instance, negative valued predictions do not make sense for binary outcomes that fall into one of two categories, and are dummy-coded as 0 or 1. Nor do negative predictions make sense for count outcomes, which take only whole-number positive values.\n",
    "\n",
    "The link function in generalized linear models maps the linear prediction equation to a function of the conditional mean which ranges from $(-∞, ∞)$. For instance, when dealing with binary outcome variables, we can use the logit link function $ln(\\frac{μ} {1 – μ})$. This maps the linear prediction equation to the log of the odds ratio, and ranges from $(-∞, ∞)$.\n",
    "\n",
    "The table below shows some of the generalized linear models that are covered by this textbook:\n",
    "\n",
    "| Model type       | Response distribution             | Range of predicted values               | Link name        | Link function         |\n",
    "|--------------------------|--------------------------|------------------------|------------------|-----------------------|\n",
    "| Linear Regression        | Normal Distribution      | real: (-∞, ∞)          | Identity         | 1 * $μ$                   |\n",
    "| Logistic Regression      | Bernoulli                | Integers: {0, 1}       | Logit          | $ln(\\frac{μ} {1 – μ})$ |\n",
    "| Poisson Regression       | Poisson Distribution     | integers: 0, 1, 2, …   | Log              | $ln(μ) $              |\n",
    "\n",
    "## The structure of this book\n",
    "\n",
    "This book will use simulated data, which cleanly illustrates the principles of fitting generalized linear models. Each page will focus on a different type of generalized linear model, and will involve data from a different hypothetical island. Each model will be explained first in the one-predictor case, using a continuous predictor, then in using several predictors, one of which will be categorical. \n",
    "\n",
    "Let's visit the islands, the links are below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Chapters\n",
    "\n",
    "1. [What are Generalized Linear Models?](1_generalized_linear_models.ipynb)\n",
    "2. [Linear Regression](2_linear_regression.ipynb)\n",
    "3. [Poisson Regression](3_poisson_regression.ipynb)\n",
    "4. [Binary Logistic Regression](4_binary_logistic_regression.ipynb)\n",
    "5. [Multinomial Logistic Regression](5_multinomial_logistic_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "By [pxr687](99_about_the_author.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
